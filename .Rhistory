base_url6 <- "https://www.consultancy.uk/jobs/page/6"
base_url7 <- "https://www.consultancy.uk/jobs/page/7"
base_url8 <- "https://www.consultancy.uk/jobs/page/8"
listings_html1 <- base_url1 %>%
read_html() %>%
html_nodes(css="table") %>%
html_table()
listings_html2 <- base_url2 %>%
read_html() %>%
html_nodes(css="table") %>%
html_table()
listings_html1
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false
library(tidyverse)
library(wbstats)
library(tictoc)
library(skimr)
library(countrycode)
library(here)
library(DBI)
library(dbplyr)
library(arrow)
library(rvest)
library(robotstxt) # check if we're allowed to scrape the data
library(scales)
library(sf)
library(readxl)
scrape_pac <- function(x){
x %>%
read_html(x)
html_table(html_nodes(x, "table"))[[1]]
mutate(str_sub(url, -4))
return(table)
}
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions2000 <- scrape_pac("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000")
contributions2020 <-scrape_pac(year2020)
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions2000 <- scrape_pac("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000")
contributions2020 <-scrape_pac(url_2020)
contributions2022 <-scrape_pac(url_2022)
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
contributions2020 <-scrape_pac(url_2020)
scrape_pac <- function(x){
x %>%
read_html(x)
html_table(html_nodes(x, "table"))[[1]]
mutate(str_sub(url, -4))
return(table)
}
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
contributions2020 <-scrape_pac(url_2020)
scrape_pac <- function(x){
x %>%
read_html(x)
html_table(html_nodes(x, "table"))[[1]]
mutate(str_sub(url, -4))
return(table)
end
}
contributions2020 <-scrape_pac(url_2020)
scrape_pac <- function(x){
x %>%
read_html(x) %>%
html_table(html_nodes(x, "table"))[[1]] %>%
mutate(str_sub(url, -4)) %>%
return(table) %>%
end
}
contributions2020 <-scrape_pac(url_2020)
scrape_pac <- function(x){
x %>%
read_html(x)
html_table(html_nodes(x, "table"))[[1]]
mutate(str_sub(url, -4))
return(table)
end
}
contributions2020 <-scrape_pac(url_2020)
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions2000 <- scrape_pac("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000")
contributions2020 <-scrape_pac(url_2020)
contributions2022 <-scrape_pac(url_2022)
url2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions2022 <-scrape_pac(url_2022)
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url_2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions2000 <- scrape_pac("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000")
contributions2020 <-scrape_pac(url_2020)
contributions2022 <-scrape_pac(url_2022)
year2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
year2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
year2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions2000 <- scrape_pac("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000")
contributions2020 <-scrape_pac(year2020)
contributions2022 <-scrape_pac(year2022)
#| label: consulting_jobs_url
#| eval: false
library(robotstxt)
paths_allowed("https://www.consultancy.uk") #is it ok to scrape?
base_url1 <- "https://www.consultancy.uk/jobs/page/1"
base_url2 <- "https://www.consultancy.uk/jobs/page/2"
base_url2 <- "https://www.consultancy.uk/jobs/page/3"
base_url4 <- "https://www.consultancy.uk/jobs/page/4"
base_url5 <- "https://www.consultancy.uk/jobs/page/5"
base_url6 <- "https://www.consultancy.uk/jobs/page/6"
base_url7 <- "https://www.consultancy.uk/jobs/page/7"
base_url8 <- "https://www.consultancy.uk/jobs/page/8"
listings_html1 <- base_url1 %>%
read_html() %>%
html_nodes(css="table") %>%
html_table()
listings_html2 <- base_url2 %>%
read_html() %>%
html_nodes(css="table") %>%
html_table()
listings_html1
listings_html1
listings_html1 <- base_url1 %>%
read_html() %>%
html_nodes(css="table") %>%
html_table()
listings_html1
base_url1 <- "https://www.consultancy.uk/jobs/page/1"
listings_html1 <- base_url1 %>%
read_html() %>%
html_nodes(css="table") %>%
html_table()
listings_html1
base_url1 %>%
read_html() %>%
html_nodes(css="table") %>%
html_table()
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false
library(tidyverse)
library(wbstats)
library(tictoc)
library(skimr)
library(countrycode)
library(here)
library(DBI)
library(dbplyr)
library(arrow)
library(rvest)
library(robotstxt) # check if we're allowed to scrape the data
library(scales)
library(sf)
library(readxl)
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false
library(robotstxt)
paths_allowed("https://www.opensecrets.org")
base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions_tables <- base_url %>%
read_html() %>%
html_nodes(css="table") %>% # this will isolate all tables on page
html_table() # Parse an html table into a dataframe
contributions <- contributions_tables[[1]] %>%
janitor::clean_names()
# write a function to parse_currency
parse_currency <- function(x){
x %>%
# remove dollar signs
str_remove("\\$") %>%
# remove all occurrences of commas
str_remove_all(",") %>%
# convert to numeric
as.numeric()
}
# clean country/parent co and contributions
contributions <- contributions %>%
separate(country_of_origin_parent_company,
into = c("country", "parent"),
sep = "/",
extra = "merge") %>%
mutate(
total = parse_currency(total),
dems = parse_currency(dems),
repubs = parse_currency(repubs)
)
contributions <- contributions %>%
mutate(year = str_sub()
scrape_pac <- function(x){
x %>%
read_html(x)
html_table(html_nodes(x, "table"))[[1]]
mutate(str_sub(url, -4))
return(table)
end
}
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url_2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions2000 <- scrape_pac("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000")
contributions2020 <-scrape_pac(url_2020)
contributions2022 <-scrape_pac(url_2022)
all_urls <- c(url_2000, url_2020, url_2022)
all_contributions <- map_df(all_urls, scrape_pac)
contributions <- contributions %>%
mutate(year = str_sub(url, -4)
scrape_pac <- function(x){
scrape_pac <- function(x){
x %>%
read_html(x)
html_table(html_nodes(x, "table"))[[1]]
mutate(str_sub(url, -4))
return(table)
end
}
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url_2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
#contributions2000 <- scrape_pac("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000")
#contributions2000 <-scrape_pac(url_2000)
contributions2020 <-scrape_pac(url_2020)
contributions2022 <-scrape_pac(url_2022)
all_urls <- c(url_2000, url_2020, url_2022)
all_contributions <- map_df(all_urls, scrape_pac)
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false
library(tidyverse)
library(wbstats)
library(tictoc)
library(skimr)
library(countrycode)
library(here)
library(DBI)
library(dbplyr)
library(arrow)
library(rvest)
library(robotstxt) # check if we're allowed to scrape the data
library(scales)
library(sf)
library(readxl)
sky_westminster <- DBI::dbConnect(
drv = RSQLite::SQLite(),
dbname = here::here("data", "sky-westminster-files.db")
)
DBI::dbListTables(sky_westminster)
#First I set up the tables from the database as dplyr compatible data frames so that I can manipulate them using R
appg_donations_db <- dplyr::tbl(sky_westminster, "appg_donations")
parties_db <- dplyr::tbl(sky_westminster, "parties")
appgs_db <- dplyr::tbl(sky_westminster, "appgs")
party_donations_db <- dplyr::tbl(sky_westminster, "party_donations")
member_appgs_db <- dplyr::tbl(sky_westminster, "member_appgs")
payments_db <- dplyr::tbl(sky_westminster, "payments")
members_db <- dplyr::tbl(sky_westminster, "members")
members_db <- members_db %>%
mutate(member_id = id)
members_payments_db <- left_join(payments_db,members_db,  by = c('member_id' = 'id'))
members_payments_db %>%
group_by(member_id, name) %>%
summarise(total = sum(value)) %>%
arrange(desc(total)) %>%
head(1)
#That slimey Theresa May received the most donations, at a cool £2.8 million - we can assume that some of this was paid by international private equity and hedge funds who were able to scoop up bargain British real estate and assets following the catastrophe of May's hard Brexit
total_value <- payments_db %>%
summarise(total = sum(value)) %>%
pull(total)
biggest_donator <- payments_db %>%
group_by(entity) %>%
summarise(total = sum(value)) %>%
mutate(proportion = total/total_value*100) %>%
filter(proportion >= 5) %>%
pull(entity)
#Yes, Withers LLP was responfible for 5.25% of donations during the period. They are an International Law Firf
members_payments_db %>%
filter(entity == biggest_donator) %>%
group_by(member_id, name) %>%
summarise(total = sum(value))
#The only MP they gave money to was Sir Geoffrey Cox, to whom they gave £1,812,732 during the period - I wonder what they received in return?
total_value <- payments_db %>%
summarise(total = sum(value)) %>%
pull(total)
biggest_donator <- payments_db %>%
group_by(entity) %>%
summarise(total = sum(value)) %>%
mutate(proportion = total/total_value*100) %>%
filter(proportion >= 5) %>%
pull(entity)
#Yes, Withers LLP was responsible for 5.25% of donations during the period. They are an International Law Firm
members_payments_db %>%
filter(entity == biggest_donator) %>%
group_by(member_id, name) %>%
summarise(total = sum(value))
#The only MP they gave money to was Sir Geoffrey Cox, to whom they gave £1,812,732 during the period - I wonder what they received in return?
distinct_donators <- members_payments_db %>%
distinct(entity) %>%
count() %>%
collect()
distinct_donators
#There are 2,213 entities that paid money to MPs
distinct_serial_donators <- members_payments_db %>%
group_by(entity) %>%
count(party_id) %>%
filter(n == 1) %>%
ungroup() %>%
count() %>%
collect()
distinct_serial_donators
#There are 934 entities that donated to more than 1 MP
proportion <- distinct_serial_donators$n/distinct_donators$n*100
proportion
#The proportion of entities that donated to more than 1 party was 69.4%
knitr::include_graphics(here::here("images", "total_donations_table.png"), error = FALSE)
parties_db
members_payments_db
join2 <-left_join(members_payments_db, parties_db, by = c('party_id' = 'id'))
table <- join2 %>%
mutate(party_name = name.y,
year = str_sub(date, -4)) %>%
group_by(party_name, year) %>%
summarise(total = sum(value)) %>%
filter(year != "2019") %>%
arrange(year) %>%
collect()
total_donations <- join2 %>%
mutate(party_name = name.y,
year = str_sub(date, -4)) %>%
filter(year != "2019") %>%
summarise(total = sum(value)) %>%
collect()
table %>%
mutate(prop = total/total_donations$total*100)
knitr::include_graphics(here::here("images", "total_donations_table.png"), error = FALSE)
table %>%
mutate(prop = total/total_donations$total*100)
knitr::include_graphics(here::here("images", "total_donations_graph.png"), error = FALSE)
table <- table %>%
arrange(desc(total))
ggplot(table) +
aes(year, total, fill = fct_reorder(party_name, -total)) +
geom_bar(stat = "identity", position = position_dodge())  +
labs(title = "Conservatives have captured the majority of political donations",
subtitle = "Donations to political parties, 2020-2022",
x = "",
y = "",
fill = "Party")
dbDisconnect(sky_westminster)
dbDisconnect(sky_westminster)
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false
library(robotstxt)
paths_allowed("https://www.opensecrets.org")
base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions_tables <- base_url %>%
read_html() %>%
html_nodes(css="table") %>% # this will isolate all tables on page
html_table() # Parse an html table into a dataframe
contributions <- contributions_tables[[1]] %>%
janitor::clean_names()
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false
library(robotstxt)
paths_allowed("https://www.opensecrets.org")
base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions_tables <- base_url %>%
read_html() %>%
html_nodes(css="table") %>% # this will isolate all tables on page
html_table() # Parse an html table into a dataframe
contributions <- contributions_tables[[1]] %>%
janitor::clean_names()
# write a function to parse_currency
parse_currency <- function(x){
x %>%
# remove dollar signs
str_remove("\\$") %>%
# remove all occurrences of commas
str_remove_all(",") %>%
# convert to numeric
as.numeric()
}
# clean country/parent co and contributions
contributions <- contributions %>%
separate(country_of_origin_parent_company,
into = c("country", "parent"),
sep = "/",
extra = "merge") %>%
mutate(
total = parse_currency(total),
dems = parse_currency(dems),
repubs = parse_currency(repubs)
)
contributions <- contributions %>%
mutate(year = str_sub(url, -4)
# write a function to parse_currency
parse_currency <- function(x){
x %>%
# remove dollar signs
str_remove("\\$") %>%
# remove all occurrences of commas
str_remove_all(",") %>%
# convert to numeric
as.numeric()
}
# clean country/parent co and contributions
contributions <- contributions %>%
separate(country_of_origin_parent_company,
into = c("country", "parent"),
sep = "/",
extra = "merge") %>%
mutate(
total = parse_currency(total),
dems = parse_currency(dems),
repubs = parse_currency(repubs)
)
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false
library(robotstxt)
paths_allowed("https://www.opensecrets.org")
base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions_tables <- base_url %>%
read_html() %>%
html_nodes(css="table") %>% # this will isolate all tables on page
html_table() # Parse an html table into a dataframe
contributions <- contributions_tables[[1]] %>%
janitor::clean_names()
#I am repeatedly getting 'Error: C stack usage is too close to the limit', even in situations where I was not getting this error code before. The code above and below was previously working and is no longer working
# write a function to parse_currency
parse_currency <- function(x){
x %>%
# remove dollar signs
str_remove("\\$") %>%
# remove all occurrences of commas
str_remove_all(",") %>%
# convert to numeric
as.numeric()
}
# clean country/parent co and contributions
contributions <- contributions %>%
separate(country_of_origin_parent_company,
into = c("country", "parent"),
sep = "/",
extra = "merge") %>%
mutate(
total = parse_currency(total),
dems = parse_currency(dems),
repubs = parse_currency(repubs)
)
contributions <- contributions %>%
mutate(year = str_sub(url, -4))
scrape_pac <- function(x){
x %>%
read_html(x)
html_table(html_nodes(x, "table"))[[1]]
mutate(str_sub(url, -4))
return(table)
end
}
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url_2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
#contributions2000 <- scrape_pac("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000")
#contributions2000 <-scrape_pac(url_2000)
contributions2020 <-scrape_pac(url_2020)
contributions2022 <-scrape_pac(url_2022)
all_urls <- c(url_2000, url_2020, url_2022)
all_contributions <- map_df(all_urls, scrape_pac)
contributions2020
#| label: consulting_jobs_url
#| eval: false
library(robotstxt)
paths_allowed("https://www.consultancy.uk") #is it ok to scrape?
base_url1 <- "https://www.consultancy.uk/jobs/page/1"
base_url2 <- "https://www.consultancy.uk/jobs/page/2"
base_url2 <- "https://www.consultancy.uk/jobs/page/3"
base_url4 <- "https://www.consultancy.uk/jobs/page/4"
base_url5 <- "https://www.consultancy.uk/jobs/page/5"
base_url6 <- "https://www.consultancy.uk/jobs/page/6"
base_url7 <- "https://www.consultancy.uk/jobs/page/7"
base_url8 <- "https://www.consultancy.uk/jobs/page/8"
listings_html1 <- base_url1 %>%
read_html() %>%
html_nodes(css="table") %>%
html_table()
listings_html2 <- base_url2 %>%
read_html() %>%
html_nodes(css="table") %>%
html_table()
listings_html1
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false
library(robotstxt)
paths_allowed("https://www.opensecrets.org")
base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
contributions_tables <- base_url %>%
read_html() %>%
html_nodes(css="table") %>% # this will isolate all tables on page
html_table() # Parse an html table into a dataframe
contributions <- contributions_tables[[1]] %>%
janitor::clean_names()
#I am repeatedly getting 'Error: C stack usage is too close to the limit', even in situations where I was not getting this error code before. The code above and below was previously working and is no longer working
